{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import sent_tokenize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=100, window_size=50):\n",
    "    \"\"\"Split a long text into multiple chunks (passages) with managable sizes.\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): Maximum size of a chunk.\n",
    "        window_size (int): Decide how many words are overlapped between two consecutive chunks. Basically #overlapped_words = chunk_size - window_size.\n",
    "    Returns:\n",
    "        str: Multiple chunks of text splitted from initial document text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "\n",
    "    while True:\n",
    "        end_idx = start_idx + chunk_size\n",
    "        chunk = \" \".join(words[start_idx:end_idx])\n",
    "        chunks.append(chunk)\n",
    "        if end_idx >= num_words:\n",
    "            break\n",
    "        start_idx += window_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def split_doc(doc, size = 325):\n",
    "    \"\"\"\n",
    "    args: A document and a chunk size\n",
    "    return: list of sub-doc that have size <= chunk size, make sure not split in the middle sentences.    \n",
    "    \"\"\"\n",
    "\n",
    "    placeholder = \"<NEWLINE>\"\n",
    "\n",
    "    context = doc.strip()\n",
    "    text_with_placeholders = context.replace(\"\\n\", f\" {placeholder} \")\n",
    "\n",
    "    words = re.split(r'[\\t ]+', text_with_placeholders)\n",
    "    result = [word if word != placeholder else \"\\n\" for word in words]\n",
    "\n",
    "    # print(result)\n",
    "    lenght = len(result)\n",
    "    index = 0\n",
    "    while index < lenght:\n",
    "        if (result[index] == \"\\n\") and (result[index + 1] == \"\\n\"):\n",
    "            lenght -= 1\n",
    "            result.pop(index+1)\n",
    "        else:\n",
    "            index += 1\n",
    "    # print(result)\n",
    "    count = 0\n",
    "    # for each in result:\n",
    "    #     if each == \"\\n\":\n",
    "    #         count += 1\n",
    "\n",
    "    # print(count)\n",
    "    # # print(result[:100])\n",
    "    chunks = []\n",
    "\n",
    "    save_idx = 0\n",
    "    while len(result) > size :\n",
    "        for i in range(size,-1,-1):\n",
    "            if result[i] == \"\\n\":\n",
    "                save_idx = i\n",
    "                break\n",
    "        # print(result[:save_idx])\n",
    "        chunks.append(\" \".join(result[:save_idx]))\n",
    "        result = result[save_idx+1:]\n",
    "    chunks.append(\" \".join(result))\n",
    "    return chunks\n",
    "\n",
    "def get_corpus(data_dir):\n",
    "    \"\"\"Transform a corpus of documents into a corpus of passages.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory that contains .txt files, each file contains text content of a wikipedia page.\n",
    "    Returns:\n",
    "        str: A corpus of chunks splitted from multiple initial documents. Each chunk will contain information about (id, title, passage)\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    meta_corpus = []\n",
    "    # data_dir = \"./all_content/\"\n",
    "    filenames = os.listdir(data_dir)\n",
    "    filenames = sorted(filenames, key=lambda x: int(re.search(r'Điều (\\d+)', x).group(1)))\n",
    "    \n",
    "    _id = 1002\n",
    "    # docs = {}\n",
    "    for filename in tqdm(filenames):\n",
    "        filepath = data_dir + filename\n",
    "        title = filename.strip(\".txt\")\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read().split(\"\\n\")\n",
    "            title = text[0]\n",
    "            text = \"\\n\".join(text[1:]).strip()\n",
    "            # docs[title] = text\n",
    "            # text = text.lstrip(title).strip()   #modify thisss........\n",
    "\n",
    "            # No overlap.\n",
    "            chunks = split_doc(text, size = 230)\n",
    "            chunks = [f\"{chunk}\" for chunk in chunks]\n",
    "            meta_chunks = [{\n",
    "                \"id\": _id + i,\n",
    "                \"len\": len(chunks[i].split()),\n",
    "                \"title\": title,\n",
    "                \"passage\": title+ \"\\n\" +chunks[i]   #modify thissss.....\n",
    "            } for i in range(len(chunks))]\n",
    "            _id += len(chunks)\n",
    "            corpus.extend(chunks)\n",
    "            meta_corpus.extend(meta_chunks)\n",
    "    return meta_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 1392.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Corpus size: 113\n",
      ">>> Example passage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 1007,\n",
       " 'len': 163,\n",
       " 'title': 'Điều 4. Giải thích từ ngữ',\n",
       " 'passage': 'Điều 4. Giải thích từ ngữ\\nNgoài các từ ngữ quy định tại Luật Chứng khoán số 54/2019/QH14, trong Nghị định này, các từ ngữ dưới đây được hiểu như sau: \\n 1. “Trái phiếu doanh nghiệp” là loại chứng khoán có kỳ hạn từ 01 năm trở lên do doanh nghiệp phát hành, xác nhận quyền và lợi ích hợp pháp của người sở hữu đối với một phần nợ của doanh nghiệp phát hành. \\n 2. “Trái phiếu doanh nghiệp xanh” là trái phiếu doanh nghiệp được phát hành để đầu tư cho dự án thuộc lĩnh vực bảo vệ môi trường, dự án mang lại lợi ích về môi trường theo quy định của pháp luật bảo vệ môi trường. \\n 3. “Trái phiếu chuyển đổi” là loại hình trái phiếu do công ty cổ phần phát hành, có thể chuyển đổi thành cổ phiếu phổ thông của chính doanh nghiệp phát hành theo điều kiện, điều khoản đã được xác định tại phương án phát hành trái phiếu.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_corpus = get_corpus(\"/home/alex/FPT/DPL302m/project/Data_mining/Data/Enterprise_Law/Final_ENT_Law/split_guide_docs/temp/6/\")\n",
    "print(f\">>> Corpus size: {len(meta_corpus)}\")\n",
    "print(f\">>> Example passage\")\n",
    "meta_corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/alex/FPT/DPL302m/project/Data_mining/Data/Enterprise_Law/Final_ENT_Law/chunk_split/final_chunk/6.jsonl\", 'w', encoding=\"utf-8\") as file:\n",
    "    json.dump(meta_corpus, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
